{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT Beginning Test for Final Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from utils_openAI import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set API key\n",
    "set_api_key(personal = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set instructions\n",
    "instruction = \"\"\" You are an undergraduate psychology student at the \n",
    "                  University of Pennsylvania who is currently participating \n",
    "                  in a research experiment. In this experiment, you will be given three\n",
    "                  different phenomena and an explanations for each of them. You are \n",
    "                  then asked to evaluate how good or bad these explanations are on \n",
    "                  a scale from -3 to +3. In your response, you will only return your\n",
    "                  evaluation numbers as follows{int1, int2, int3}, nothing else. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phenomonon = \"\"\" Phenomenon: Subjects were asked to imagine a series of objects that \n",
    "were make-believe (for example, a unicorn) or that were real but not present in the \n",
    "room (for example, a mountain). As the subjects created mental images of the various \n",
    "objects, they were asked questions about their images and told to respond as quickly \n",
    "as possible, without reflecting on their answers. They were also asked the same \n",
    "questions about objects they could actually see in the room (for example, a pen). \n",
    "From an analysis of the responses to these questions and of the times it took \n",
    "subjects to respond, the researchers found a similar pattern of responses and \n",
    "response times for all three types of objects.\"\"\"\n",
    "\n",
    "explanation = \"\"\" Patterns of activation in the vision area of the brain led \n",
    "researchers to conclude this happens because imagining objects uses the same \n",
    "process as seeing objects. \"\"\"\n",
    "\n",
    "prompt = \"Phenomenon: [{}] Explanation: [{}]\".format(phenomonon, explanation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output\n",
    "response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "#print the result\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First look at data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv into pd\n",
    "df = pd.read_csv('NeuroExp1.csv')\n",
    "df.shape\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by condition and get mean of Answers and the standard deviation\n",
    "\n",
    "c1 = df[df['Condition'] == 'Without Neuroscience Long']\n",
    "print(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good = df[df['Answer3 Good'] == True]['Answer3'].mean()\n",
    "bad = df[df['Answer3 Good'] == False]['Answer3'].mean()\n",
    "print(f'Good: {good}, Bad: {bad}')\n",
    "\n",
    "\n",
    "good = []\n",
    "bad = []\n",
    "#loop over pandas df\n",
    "for i in len(df): \n",
    "    if df.iloc[i]['Answer1 Good']:\n",
    "        good.append(df.iloc[i]['Answer1'])\n",
    "    else: \n",
    "        bad.append(df.iloc[i]['Answer1'])\n",
    "    \n",
    "    if df.iloc[i]['Answer2 Good']:\n",
    "        good.append(df.iloc[i]['Answer2'])\n",
    "    else:\n",
    "        bad.append(df.iloc[i]['Answer2'])\n",
    "    \n",
    "    if df.iloc[i]['Answer3 Good']:\n",
    "        good.append(df.iloc[i]['Answer3'])\n",
    "    else:\n",
    "        bad.append(df.iloc[i]['Answer3'])\n",
    "\n",
    "print(good.mean(), bad.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through pandas and if answer 1 good is true, add to good list, else add to bad list  \n",
    "#do the same for answer 2 and 3\n",
    "#take the mean of each list and compare them\n",
    "def get_good_bad(df):\n",
    "    good = np.array([])\n",
    "    bad = np.array([])\n",
    "    #loop over pandas df\n",
    "    for i in range(len(df)):\n",
    "        if df.iloc[i]['Answer1 Good'] == True:\n",
    "            #add to numpy array \n",
    "\n",
    "            good = np.append(good, df.iloc[i]['Answer1'])\n",
    "        else: \n",
    "            bad = np.append(bad, df.iloc[i]['Answer1'])\n",
    "        \n",
    "        if df.iloc[i]['Answer2 Good'] ==True:\n",
    "            good = np.append(good, df.iloc[i]['Answer2'])\n",
    "        else:\n",
    "            bad =  np.append(bad,df.iloc[i]['Answer2'])\n",
    "        \n",
    "        if df.iloc[i]['Answer3 Good'] == True:\n",
    "            good = np.append(good, df.iloc[i]['Answer3'])\n",
    "        else:\n",
    "            bad =  np.append(bad, df.iloc[i]['Answer3'])\n",
    "    return good, bad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference of good and bad explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good, bad = get_good_bad(df)\n",
    "#plt.bar(['Good', 'Bad'], [good.mean(), bad.mean()], yerr = [good.std(), bad.std()])\n",
    "#plt.show()\n",
    "\n",
    "#create a box plot of good and bad\n",
    "plt.boxplot([good, bad])\n",
    "\n",
    "#add the mean and std as text to the plot\n",
    "plt.text(0.9, -3.25, 'Mean: {}'.format(round(good.mean(), 2)))\n",
    "#plt.text(1, good.mean() + good.std(), 'Std: {}'.format(good.std()))\n",
    "plt.text(1.9, -3.25, 'Mean: {}'.format(round(bad.mean(), 2)))\n",
    "#plt.text(2, bad.mean() + bad.std(), 'Std: {}'.format(bad.std()))\n",
    "#make background light blue background color and a different layout\n",
    "plt.style.use('bmh')\n",
    "#add a title to the figure\n",
    "plt.title(\"Comparrison of GPT 3.5's scoring of explanations\")\n",
    "#replace x axis text with Good and Bad\n",
    "plt.xticks([1, 2], ['Good', 'Bad'])\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference per condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a bar plot where the mean of good and bad are plotted within each condition\n",
    "#group by condition and get mean of Answers and the standard deviation\n",
    "c1 = df[df['Condition'] == 'Without Neuroscience Long']\n",
    "c2 = df[df['Condition'] == 'Without Neuroscience Short']\n",
    "c3 = df[df['Condition'] == 'With Neuroscience Long']\n",
    "c4 = df[df['Condition'] == 'With Neuroscience Short']\n",
    "\n",
    "#get list of scors of each condition\n",
    "goodc1, badc1 = get_good_bad(c1)\n",
    "goodc2, badc2 = get_good_bad(c2)\n",
    "goodc3, badc3 = get_good_bad(c3)\n",
    "goodc4, badc4 = get_good_bad(c4)\n",
    "\n",
    "#create box plots with good and bad of each condition next to eachother\n",
    "plt.boxplot([goodc1, badc1, goodc2, badc2, goodc3, badc3, goodc4, badc4])\n",
    "#add the mean and std as text to the plot\n",
    "plt.text(0.75, -3.25, 'M: {}'.format(round(goodc1.mean(), 2)))\n",
    "plt.text(1.75, -3.25, 'M: {}'.format(round(badc1.mean(), 2)))\n",
    "plt.text(2.75, -3.25, 'M: {}'.format(round(goodc2.mean(), 2)))\n",
    "plt.text(3.75, -3.25, 'M: {}'.format(round(badc2.mean(), 2)))\n",
    "plt.text(4.75, -3.25, 'M: {}'.format(round(goodc3.mean(), 2)))\n",
    "plt.text(5.75, -3.25, 'M: {}'.format(round(badc3.mean(), 2)))\n",
    "plt.text(6.75, -3.25, 'M: {}'.format(round(goodc4.mean(), 2)))\n",
    "plt.text(7.75, -3.25, 'M: {}'.format(round(badc4.mean(), 2)))\n",
    "#add the condition names as text to the plot\n",
    "plt.text(1, 3.1, 'Without Neuro Long') \n",
    "plt.text(3, 3.1, 'Without Neuro Short') \n",
    "plt.text(5, 3.1, 'With Neuro Long') \n",
    "plt.text(7, 3.1, 'With Neuro Short') \n",
    "plt.style.use('seaborn')\n",
    "#add a title to the figure\n",
    "#plt.title(\"Comparrison of GPT 3.5's scoring of explanations\")\n",
    "#make background light blue and take away the grid\n",
    "plt.style.use('seaborn')\n",
    "#color the boxplots of each condition differently\n",
    "plt.xticks([1, 2, 3, 4, 5, 6, 7, 8], ['Good', 'Bad', 'Good', 'Bad', 'Good', 'Bad', 'Good', 'Bad'])\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0dc0b812dbb6b39e83a0b36e1e73ba7b4394f8620cbcc072f1b63b4ffecb41d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
