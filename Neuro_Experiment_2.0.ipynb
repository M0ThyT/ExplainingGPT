{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2 of the NeuroScienceJargon Paper\n",
    "This paper shows the experiment conducted to kind of replicate the results of the paper: Deconstructing the seductive allure of neuroscience explanations. \n",
    "In this experiment, subjects are always given one good and one bad explanation for a phenomenon. There are three conditions: \n",
    "* Both explanations containing neuroscience jargon\n",
    "* No explanation containing neuroscience jargon\n",
    "* Only the bad explanation containing neuroscience jargon\n",
    "\n",
    "subjects are asked to pick the explanation they find more satisfying, and are asked to explain in one or two sentences why they picked that explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_openAI import *\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) #for nicer output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the Stimulus and the explanations ready first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulus = ['Babies were seated on their mothers’ laps in front of a stage. Researchers used a camera to track where the babies were looking. The babies saw a hand reach out and place one doll on the stage. Then a screen was raised, hiding the doll. A hand reached out again and placed a second doll on the stage, out of sight behind the screen. Then the screen dropped. In some cases, there were two dolls on the stage, as there should be, and in some cases there was only one doll. The researchers found that the babies looked much longer at the stage when there was only one doll than when there were two dolls. This looking-time difference between one doll and two dolls lead the researchers to conclude that babies can calculate 1 + 1 = 2.', \n",
    "'Subjects sat at a computer screen. They saw a rapidly flashing series of pictures of faces. Somewhere in this series of faces there were two pictures of houses. Subjects had to press a button each time they saw a house. When the two houses were far apart in the sequence, the subjects were very good at this task. But when the houses were presented close together in the sequence, subjects failed to press the button for the second house. The researchers call this phenomenon “attentional blink.”', \n",
    "'Researchers recruited equal numbers of male and female participants. The participants took a series of spatial  reasoning tasks and were interviewed. The researchers determined that men are better at spatial reasoning in general. From the interviews, they discovered that the men had played more sports in their childhood on average than the women.']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explanation\n",
    "#Without Neuroscience Short Good\n",
    "without_good = [\"The researchers claim this happens because the babies had formed an expectation about how many dolls there should be on the stage. The babies knew there should be two dolls, and their surprise at seeing only one led to their looking longer.\",\n",
    "      \"The researchers claim that this phenomenon occurs because the subjects were still processing the first house and missed seeing the second house because they did not have enough attentional resources left.\", \n",
    "      \"The researchers conclude that the difference in involvement in sports explains the gender difference in spatial reasoning abilities.\"]\n",
    "\n",
    "#Without Neuroscience Short Bad\n",
    "without_bad = [\"The researchers claim this happens because the amount of time the babies spent looking at the stage is directly proportional to how much they liked the display. The researchers used this timing data to calculate babies’ preference for the single doll.\", \n",
    "      \"The researchers claim that this phenomenon occurs because the second house appeared later in the sequence than the first house, and this temporal relationship between the two houses caused the attentional blink.\", \n",
    "      \"The researchers conclude that women’s poor performance relative to men’s explains the gender difference in spatial reasoning abilities.\"]\n",
    "\n",
    "\n",
    "#With Neuroscience Short Good\n",
    "with_good = [\"Scans of the babies’ brains show that the parietal lobe, known to be involved in math, governed the babies’ expectations about how many dolls there should be. They expected two, so they were surprised to see one, so they looked longer.\", \n",
    "      \"Researchers concluded that this occurs because of frontal lobe areas, previously shown to mediate attention. Subjects were still processing the first house and missed the second because they had insufficient attentional resources.\", \n",
    "      \"Brain scans of the right premotor area, known to be involved in spatial tasks, indicate that the difference in sports involvement explains this gender difference.\"]\n",
    "\n",
    "\n",
    "#With Neuroscience Short Bad\n",
    "with_bad = [\"Scans of the babies’ brains show that the parietal lobe, known to be involved in math, governed how long babies looked at the stage. Researchers used this timing data, which is proportional to babies’ liking of the display, to calculate their preferences.\", \n",
    "      \"Researchers concluded that this occurs because of frontal lobe areas, previously shown to mediate attention. The second house appeared later in the sequence. This temporal relationship between the two houses caused the attentional blink.\", \n",
    "      \"Brain scans of the right premotor area, known to be involved in spatial tasks, indicate that women’s poor performance relative to men’s explains this gender difference.\"]\n",
    "\n",
    "\n",
    "\n",
    "#make into a dict of lists\n",
    "explanations_s2 = {\"Without Neuroscience Short Good\":without_good, \"Without Neuroscience Short Bad\":without_bad,  \"With Neuroscience Short Good\":with_good, \"With Neuroscience Short Bad\":with_bad}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test whether the functions work to create the prompts and conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Subjects sat at a computer screen. They saw a rapidly flashing series of pictures of faces. Somewhere in this series of faces there were two pictures of houses. Subjects had to press a button each time they saw a house. When the two houses were far apart in the sequence, the subjects were very good at this task. But when the houses were presented close together in the sequence, subjects failed to press the button for the second house. The researchers call this phenomenon “attentional blink.”', 'The researchers claim that this phenomenon occurs because the subjects were still processing the first house and missed seeing the second house because they did not have enough attentional resources left.', 'Researchers concluded that this occurs because of frontal lobe areas, previously shown to mediate attention. The second house appeared later in the sequence. This temporal relationship between the two houses caused the attentional blink.'] Mixed 1 True\n"
     ]
    }
   ],
   "source": [
    "l, c, n, g = make_condition_exp2(stimulus, explanations_s2)\n",
    "print(l, c, n, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####Phenomenon#####:\n",
      "Subjects sat at a computer screen. They saw a rapidly flashing series of pictures of faces. Somewhere in this series of faces there were two pictures of houses. Subjects had to press a button each time they saw a house. When the two houses were far apart in the sequence, the subjects were very good at this task. But when the houses were presented close together in the sequence, subjects failed to press the button for the second house. The researchers call this phenomenon “attentional blink.”\n",
      "#####Explanation 1#####:\n",
      "The researchers claim that this phenomenon occurs because the subjects were still processing the first house and missed seeing the second house because they did not have enough attentional resources left.\n",
      "#####Explanation 2#####:\n",
      "Researchers concluded that this occurs because of frontal lobe areas, previously shown to mediate attention. The second house appeared later in the sequence. This temporal relationship between the two houses caused the attentional blink.\n"
     ]
    }
   ],
   "source": [
    "print(create_prompt_exp2(l, g))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the instructions for GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = '''You are a research subject participating in an experiment where you will be given a phenomenon and 2 explanations for the phenomenon. \n",
    "The prompt will show you the text \"#####Phenomenon#####:\" before the phenomenon,  \"#####Explanation 1#####:\" before the first explanation, and \n",
    "#####Explanation 2#####: before the second explanation. Your task is to pick the explanation that you find more satisfying. To do that, please answer with\n",
    "[1] if you prefer explanation number 1, and [2] if you prefer explanation number 2, and [0] if you find the two explanations equally satisfying. \n",
    "After you have made your choice, please also explain in one or two sentences why you made that choice. \n",
    "Return your answer in the following format: [int]-Explanation why you chose that answer.'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the Experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n",
      "CSS API key\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 470ee95236207b5f929a4f18e230bbfa in your message.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dv/nfxpqfgn50ncwmnyg61n91m80000gn/T/ipykernel_36984/4103839657.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpersonal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#get answer from GPT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     response = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         messages=[\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         )\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m             return (\n\u001b[0;32m--> 619\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    620\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    680\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    683\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 470ee95236207b5f929a4f18e230bbfa in your message.)"
     ]
    }
   ],
   "source": [
    "#create empty pandas with columns stimulus_number, condition, good_first, answer \n",
    "df = pd.DataFrame(columns = ['stimulus_number', 'condition', 'good_first', 'answer'])\n",
    "for i in range(400):\n",
    "    #create condition\n",
    "    cond_list, condition, num_stimulus, good_first = make_condition_exp2(stimulus, explanations_s2)\n",
    "    #create prompt\n",
    "    prompt = create_prompt_exp2(cond_list, good_first)\n",
    "\n",
    "    #set OPEN ai API key\n",
    "    openai.api_key = get_api_key(personal = False)\n",
    "    #get answer from GPT\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": instruction},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ], \n",
    "        timeout = 60, #set timeout to 60 seconds\n",
    "        temperature = 0.2 #set temperature to 0.2 to have little randomless only\n",
    "    )\n",
    "    #get response\n",
    "    answer = response['choices'][0]['message']['content']\n",
    "\n",
    "    #create a new pd df with 1 row as the one outside the loop with values filled in\n",
    "    df_temp = pd.DataFrame([[num_stimulus, condition, good_first, answer]], columns = ['stimulus_number', 'condition', 'good_first', 'answer'])\n",
    "    #append to df\n",
    "    df = pd.concat([df, df_temp])\n",
    "    #save csv file after every itteration\n",
    "    #df.to_csv(\"NeuroExp2_1.csv\") #stopped working after 153\n",
    "    #df.to_csv(\"NeuroExp2_2.csv\") #stopped working after 115\n",
    "    #df.to_csv(\"NeuroExp2_3.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv\n",
    "neuro_1 = pd.read_csv(\"NeuroExp2_1.csv\")\n",
    "neuro_2 = pd.read_csv(\"NeuroExp2_2.csv\")\n",
    "neuro_3 = pd.read_csv(\"NeuroExp2_3.csv\")\n",
    "\n",
    "neuro = pd.concat([neuro_1, neuro_2, neuro_3])\n",
    "len(neuro)\n",
    "#save csv\n",
    "neuro.to_csv(\"NeuroExp2.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
